{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 2610,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.05747126436781609,
      "grad_norm": 2.3995840549468994,
      "learning_rate": 4.906130268199234e-05,
      "loss": 1.9954,
      "step": 50
    },
    {
      "epoch": 0.11494252873563218,
      "grad_norm": 1.7351502180099487,
      "learning_rate": 4.810344827586207e-05,
      "loss": 1.8579,
      "step": 100
    },
    {
      "epoch": 0.1724137931034483,
      "grad_norm": 1.5839242935180664,
      "learning_rate": 4.71455938697318e-05,
      "loss": 1.8012,
      "step": 150
    },
    {
      "epoch": 0.22988505747126436,
      "grad_norm": 1.7903965711593628,
      "learning_rate": 4.618773946360154e-05,
      "loss": 1.7288,
      "step": 200
    },
    {
      "epoch": 0.28735632183908044,
      "grad_norm": 2.2031588554382324,
      "learning_rate": 4.5229885057471264e-05,
      "loss": 1.7038,
      "step": 250
    },
    {
      "epoch": 0.3448275862068966,
      "grad_norm": 1.5923446416854858,
      "learning_rate": 4.4272030651341e-05,
      "loss": 1.668,
      "step": 300
    },
    {
      "epoch": 0.40229885057471265,
      "grad_norm": 1.5097861289978027,
      "learning_rate": 4.331417624521073e-05,
      "loss": 1.6523,
      "step": 350
    },
    {
      "epoch": 0.45977011494252873,
      "grad_norm": 2.2423198223114014,
      "learning_rate": 4.235632183908046e-05,
      "loss": 1.6291,
      "step": 400
    },
    {
      "epoch": 0.5172413793103449,
      "grad_norm": 3.989196538925171,
      "learning_rate": 4.1398467432950195e-05,
      "loss": 1.5954,
      "step": 450
    },
    {
      "epoch": 0.5747126436781609,
      "grad_norm": 3.809201717376709,
      "learning_rate": 4.0440613026819925e-05,
      "loss": 1.575,
      "step": 500
    },
    {
      "epoch": 0.632183908045977,
      "grad_norm": 4.44716739654541,
      "learning_rate": 3.9482758620689656e-05,
      "loss": 1.5675,
      "step": 550
    },
    {
      "epoch": 0.6896551724137931,
      "grad_norm": 2.334320306777954,
      "learning_rate": 3.852490421455939e-05,
      "loss": 1.52,
      "step": 600
    },
    {
      "epoch": 0.7471264367816092,
      "grad_norm": 4.961260795593262,
      "learning_rate": 3.7567049808429125e-05,
      "loss": 1.5423,
      "step": 650
    },
    {
      "epoch": 0.8045977011494253,
      "grad_norm": 2.0789504051208496,
      "learning_rate": 3.660919540229885e-05,
      "loss": 1.5106,
      "step": 700
    },
    {
      "epoch": 0.8620689655172413,
      "grad_norm": 3.382378578186035,
      "learning_rate": 3.5651340996168586e-05,
      "loss": 1.4713,
      "step": 750
    },
    {
      "epoch": 0.9195402298850575,
      "grad_norm": 2.5222082138061523,
      "learning_rate": 3.469348659003832e-05,
      "loss": 1.4551,
      "step": 800
    },
    {
      "epoch": 0.9770114942528736,
      "grad_norm": 2.6203460693359375,
      "learning_rate": 3.373563218390805e-05,
      "loss": 1.447,
      "step": 850
    },
    {
      "epoch": 1.0344827586206897,
      "grad_norm": 2.6676087379455566,
      "learning_rate": 3.277777777777778e-05,
      "loss": 1.4249,
      "step": 900
    },
    {
      "epoch": 1.0919540229885056,
      "grad_norm": 2.959787368774414,
      "learning_rate": 3.181992337164751e-05,
      "loss": 1.4322,
      "step": 950
    },
    {
      "epoch": 1.1494252873563218,
      "grad_norm": 3.4191861152648926,
      "learning_rate": 3.086206896551724e-05,
      "loss": 1.3947,
      "step": 1000
    },
    {
      "epoch": 1.206896551724138,
      "grad_norm": 4.248751640319824,
      "learning_rate": 2.990421455938697e-05,
      "loss": 1.3656,
      "step": 1050
    },
    {
      "epoch": 1.264367816091954,
      "grad_norm": 3.235461950302124,
      "learning_rate": 2.894636015325671e-05,
      "loss": 1.3902,
      "step": 1100
    },
    {
      "epoch": 1.3218390804597702,
      "grad_norm": 3.2282209396362305,
      "learning_rate": 2.7988505747126436e-05,
      "loss": 1.4185,
      "step": 1150
    },
    {
      "epoch": 1.3793103448275863,
      "grad_norm": 4.3904337882995605,
      "learning_rate": 2.703065134099617e-05,
      "loss": 1.3821,
      "step": 1200
    },
    {
      "epoch": 1.4367816091954024,
      "grad_norm": 3.66037917137146,
      "learning_rate": 2.60727969348659e-05,
      "loss": 1.3594,
      "step": 1250
    },
    {
      "epoch": 1.4942528735632183,
      "grad_norm": 6.206366062164307,
      "learning_rate": 2.5114942528735635e-05,
      "loss": 1.3365,
      "step": 1300
    },
    {
      "epoch": 1.5517241379310345,
      "grad_norm": 5.429590702056885,
      "learning_rate": 2.4157088122605366e-05,
      "loss": 1.3152,
      "step": 1350
    },
    {
      "epoch": 1.6091954022988506,
      "grad_norm": 3.5656683444976807,
      "learning_rate": 2.3199233716475097e-05,
      "loss": 1.3227,
      "step": 1400
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 7.7709760665893555,
      "learning_rate": 2.2241379310344828e-05,
      "loss": 1.2925,
      "step": 1450
    },
    {
      "epoch": 1.7241379310344827,
      "grad_norm": 3.274650812149048,
      "learning_rate": 2.1283524904214562e-05,
      "loss": 1.2981,
      "step": 1500
    },
    {
      "epoch": 1.7816091954022988,
      "grad_norm": 3.7964072227478027,
      "learning_rate": 2.0325670498084293e-05,
      "loss": 1.3201,
      "step": 1550
    },
    {
      "epoch": 1.839080459770115,
      "grad_norm": 4.5366668701171875,
      "learning_rate": 1.9367816091954024e-05,
      "loss": 1.3262,
      "step": 1600
    },
    {
      "epoch": 1.896551724137931,
      "grad_norm": 3.9171910285949707,
      "learning_rate": 1.8409961685823758e-05,
      "loss": 1.2966,
      "step": 1650
    },
    {
      "epoch": 1.9540229885057472,
      "grad_norm": 3.4085676670074463,
      "learning_rate": 1.745210727969349e-05,
      "loss": 1.2762,
      "step": 1700
    },
    {
      "epoch": 2.0114942528735633,
      "grad_norm": 4.7024455070495605,
      "learning_rate": 1.649425287356322e-05,
      "loss": 1.281,
      "step": 1750
    },
    {
      "epoch": 2.0689655172413794,
      "grad_norm": 5.345009803771973,
      "learning_rate": 1.553639846743295e-05,
      "loss": 1.281,
      "step": 1800
    },
    {
      "epoch": 2.1264367816091956,
      "grad_norm": 4.323765754699707,
      "learning_rate": 1.4578544061302681e-05,
      "loss": 1.2541,
      "step": 1850
    },
    {
      "epoch": 2.1839080459770113,
      "grad_norm": 6.367568016052246,
      "learning_rate": 1.3620689655172414e-05,
      "loss": 1.2534,
      "step": 1900
    },
    {
      "epoch": 2.2413793103448274,
      "grad_norm": 4.700084686279297,
      "learning_rate": 1.2662835249042146e-05,
      "loss": 1.271,
      "step": 1950
    },
    {
      "epoch": 2.2988505747126435,
      "grad_norm": 3.871269702911377,
      "learning_rate": 1.1704980842911879e-05,
      "loss": 1.273,
      "step": 2000
    },
    {
      "epoch": 2.3563218390804597,
      "grad_norm": 4.327485084533691,
      "learning_rate": 1.074712643678161e-05,
      "loss": 1.2477,
      "step": 2050
    },
    {
      "epoch": 2.413793103448276,
      "grad_norm": 4.248457908630371,
      "learning_rate": 9.789272030651342e-06,
      "loss": 1.231,
      "step": 2100
    },
    {
      "epoch": 2.471264367816092,
      "grad_norm": 6.0463361740112305,
      "learning_rate": 8.831417624521075e-06,
      "loss": 1.2402,
      "step": 2150
    },
    {
      "epoch": 2.528735632183908,
      "grad_norm": 4.955642223358154,
      "learning_rate": 7.873563218390804e-06,
      "loss": 1.2334,
      "step": 2200
    },
    {
      "epoch": 2.586206896551724,
      "grad_norm": 5.1024065017700195,
      "learning_rate": 6.915708812260536e-06,
      "loss": 1.1941,
      "step": 2250
    },
    {
      "epoch": 2.6436781609195403,
      "grad_norm": 4.594698429107666,
      "learning_rate": 5.957854406130269e-06,
      "loss": 1.235,
      "step": 2300
    },
    {
      "epoch": 2.7011494252873565,
      "grad_norm": 5.566127777099609,
      "learning_rate": 5e-06,
      "loss": 1.2366,
      "step": 2350
    },
    {
      "epoch": 2.7586206896551726,
      "grad_norm": 3.7259409427642822,
      "learning_rate": 4.042145593869732e-06,
      "loss": 1.1844,
      "step": 2400
    },
    {
      "epoch": 2.8160919540229887,
      "grad_norm": 3.7263729572296143,
      "learning_rate": 3.0842911877394637e-06,
      "loss": 1.2551,
      "step": 2450
    },
    {
      "epoch": 2.873563218390805,
      "grad_norm": 5.45544958114624,
      "learning_rate": 2.1264367816091954e-06,
      "loss": 1.2252,
      "step": 2500
    },
    {
      "epoch": 2.9310344827586206,
      "grad_norm": 3.248558521270752,
      "learning_rate": 1.1685823754789273e-06,
      "loss": 1.209,
      "step": 2550
    },
    {
      "epoch": 2.9885057471264367,
      "grad_norm": 6.368236541748047,
      "learning_rate": 2.1072796934865898e-07,
      "loss": 1.2579,
      "step": 2600
    }
  ],
  "logging_steps": 50,
  "max_steps": 2610,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 26570733815808.0,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
